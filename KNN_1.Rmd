---
title: 
author: 
date: 9/29/2020
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Problem 1 (KNN)
Consider the training and test data posted on Canvas in the files `training_data_knn_mp1.csv` and  `test_data_knn_mp1.csv`, respectively, for a classification problem with two classes.  

## a 
Fit KNN with $K = 1, 2, 3,\ldots, 35,50,100,150$.    

**Answer**:    
Done.      

## b   
Plot training and test error rates against $1/K$. Explain what you observe. Is it consistent with what you expect from the class?     

**Answer**:    
It is observed that the training error rate decreases as the model becomes more flexible. However, the test error rate has a U shape. After it hits the optimal points, the error rate increases.This is consistent with what we observe from class.    
   
## c    
What is the optimal value of $K$ (if there are several such $K$'s pick the smallest one)? What are the training and test error rates associated with the optimal $K$?     

**Answer**:    
the optimal value of $K$ are K=33,34,50. Because they have the smallest test error rates. We select the smallest one, 33, because it is the most flexible one, as there is an inverse relationship between flexibility and the size of K ($1/K$). Training and test error rates associated with the optimal $K$ respectively are: 0.1866667 and 0.1866667.

## d    
Make a plot of the training data that also shows the decision boundary for the optimal $K$ and the _Bayes_ decision boundary from problem 6 on HW 1. Comment on what you observe. Does the decision boundary seem sensible when compared to the Bayes decision boundary?     
HW-1_Problem-6. Let $X=(X1,X2)∈[0,1]×[0,1]$ and $Y∼Bernoulli(p=X1⋅X2)$. Plot the Bayes decision boundary ${(x1,x2):P(Y=1|X=(x1,x2))=0.5}$ and indicate the regions in $[0,1]×[0,1]$ whose points would be classified as 0 and 1.      

**Answer**:    
The Bayes' decision boundary is very close to that of the KNN with optimal K at 33. Yes its sensible because, the Bayes' classifier is the best classifier and if the KNN boundary is close to it then, it makes the decision boundary for the KNN pretty good.


```{r}
library(ggplot2)
# Load training data
train_data = read.csv('training_data_knn_mp1.csv',header = TRUE)
head(train_data)
```
```{r}
ggplot(data=train_data,aes(x=x.1,y=x.2,color=y))+
  geom_point()+
  labs(color='y')
```
```{r}
# Load test data
test_data=read.csv('test_data_knn_mp1.csv',header = TRUE)

# Apply KNN
library(class)

# Fit KNN with K = 1,2,...,35,50,100,150

# Form sequence of K's 
K=c(seq(1, 35, by = 1) ,50,100,150) # K=seq(1, 30, by = 1)   
# Define variables to record 
train_err_rate=test_err_rate=c()

for (i in 1:length(K)) {
  set.seed(123)
  # Fit KNN for K_i and evaluate its performance on training data
  mod_train <- knn(train=train_data[,1:2], test=train_data[,1:2], cl=train_data[,3], k = K[i])
  # Fit KNN for K_i and evaluate its performance on test data
  mod_test <- knn(train_data[,1:2], test_data[,1:2], train_data[,3], k = K[i])
  # Record error rate for training data
  train_err_rate[i] <- mean(mod_train != train_data[,3])
  # Record error rate for test data
  test_err_rate[i] <- mean(mod_test != test_data[,3])
}
```


```{r}
# Plot training and test error rates against 1/K. 
result=data.frame(K,flexibility=1/K,train_err_rate,test_err_rate)
ggplot(result)+
  geom_line(aes(flexibility,train_err_rate,color="train"))+
  geom_line(aes(flexibility,test_err_rate,color="test"))+
  geom_vline(xintercept = (1/K)[which.min(test_err_rate)],color = "blue", size=0.5,linetype="dotted")+
  labs(x='1/K (Flexibility)',y='error rate',color='Color')

```

```{r}
# Finding the optimal value of K based on test error rate.
result[test_err_rate == min(result$test_err_rate),]

# Making a plot of the training data that also shows the decision boundary for the optimal K

# Form grid
n.grid <- 100
x1.grid <- seq(from = min(train_data[,1:2][, 1]), to = max(train_data[,1:2][, 1]), length.out = n.grid)
x2.grid <- seq(from = min(train_data[,1:2][, 2]), to = max(train_data[,1:2][, 2]), length.out = n.grid)
grid <- expand.grid(x1.grid, x2.grid)
colnames(grid) <- c('x.1','x.2')
```

```{r}
# Finding the optimal value of K based on test error rate.
result[test_err_rate == min(result$test_err_rate),]
```

```{r}
# Making a plot of the training data that also shows the decision boundary for the optimal K

# Form grid
n.grid <- 100
x1.grid <- seq(from = min(train_data[,1:2][, 1]), to = max(train_data[,1:2][, 1]), length.out = n.grid)
x2.grid <- seq(from = min(train_data[,1:2][, 2]), to = max(train_data[,1:2][, 2]), length.out = n.grid)
grid <- expand.grid(x1.grid, x2.grid)
colnames(grid) <- c('x.1','x.2')

# Record optimal value of K
K_opt = result[test_err_rate == min(result$test_err_rate),'K']
# Fit KNN for optimal K and evaluate its performance on points belonging to grid
mod_opt <- knn(train_data[,1:2], grid, train_data[,3], k = K_opt, prob = T)
prob <- attr(mod_opt, "prob") # prob is voting fraction for winning class
prob <- ifelse(mod_opt == "1", prob, 1 - prob) # now it is voting fraction for class='yes'
prob <- matrix(prob, nrow=n.grid, ncol=n.grid) # form a matrix of probabilities
```

```{r}
print("We have these optimal Ks as follow:")
print(K_opt)
Optimal_K = match( min(test_err_rate) , test_err_rate )
print(paste0( "The the optimal value of K, i.e. the K associated with min(test_err_rate), is:  K=", Optimal_K ) )
print(paste0( "The test error rate associated with k=", toString(Optimal_K), " is ", min(test_err_rate), "." ) )
print(paste0( "The train error rate associated with k=", toString(Optimal_K), " is ", toString(train_err_rate[Optimal_K]) , "." ) )

main_title = paste( "min(test_err_rate)=", 
                    toString( min(test_err_rate) ), 
                    "for K=", 
                    toString( match( min(test_err_rate) , test_err_rate ) ) )

#Note: main_title added to below plot() as ", main=main_title"
```


```{r}
plot(train_data[,1:2], col = ifelse(train_data[,3] == "1", 'blue', 'red'),pch=20, main=main_title) # Added from above chunk: , main=main_title
contour(x1.grid, x2.grid, prob, levels = 0.5, labels = "", xlab = "", ylab = "", 
         main = "", lwd=1.5,add = T)

x2.grid <- 1/(2*x1.grid)   #Bayes boundary
lines(x1.grid, x2.grid, type="l", col="green")

legend("topright", legend = c('1', '0'),col=c('blue', 'red'),cex=0.7,pch=20)
```

##### Problem 2 (Modified KNN)
**Note**: The standard Euclidean norm on $\mathbb{R}^n$ is defined by $\|x\|_2 = \sqrt{x.x}$. There are other useful norms, as we'll see.    

Let $x$ be a point in $\mathbb{R}^d$ (in our case $d$=2). Using the usual KNN, we would assign a class label to $x$ based on the majority vote. On this problem, we'll introduce an alternative decision rule which takes into account the distances between $x$ and its $K$ nearest neighbors. 

Let $x_1,x_2,\ldots,x_K$ be the $K$ nearest neighbors of $x$ that belong to the training set (if there are ties for the $K$th nearest
neighbor then all those points are included) and let $y_1,y_2,\ldots,y_K$ be their class labels (in our case 0 or 1). 

Let $N_0=|\{i:y_i=0\}|$ and $N_1=|\{i:y_i=1\}|$. Note that $N_0+N_1=K$.

__Decision rule__:        
1. If $N_0=0$ then assign class 1 to $x$.      
2. If $N_1=0$ then assign class 0 to $x$.      
3. If $N_0>0$ and $N_1>0$ then assign class 1 to $x$ if      
$$
\frac{1}{N_1}\sum_{i:y_i=1} \|x-x_i\|_2 < \frac{1}{N_0}\sum_{i:y_i=0} \|x-x_i\|_2
$$
and class 0 if the above inequality sign is flipped. If the two sums are equal, break the ties at random.

**Example**: Assume that we want to know the class of the point $(x_{i},y_{i})$ that has 5 neighbors as follow:    
K=5      
the first fifth distances are 1,2,3,4,5.        
Their class respectively are: 1,1,0,0,0.     
Based on the simple KNN the class of this point would be 0.     
But based on the modified KNN the class of this point is 1 as follows:
$$\frac{1}{2} * (1+2) < \frac{1}{3} * (3+4+5)$$     
$$\frac{3}{2} < \frac{12}{3})$$     
$$1.5 < 4$$      
So, the class of the point $(x_{i},y_{i})$ is 1.

Write a function `modified_knn()` which receives the same set of arguments as the usual `knn()` function (`train`, `test`, `cl` and `k`) from the `class` package and outputs a sequence of predicted classes using the new decision rule. Repeat parts (a)-(c) of problem 1 for the modified KNN and compare the test error rates associated with the optimal $K$'s of the two approaches. 

```{r}
modified_knn <- function(train, test, cl, k){
  
  set.seed(1)
  dist <- c()
  test.cl <- c()
  
  euclideanDist <- function(a, b){
      d = 0
      for(i in c(1:length(a))) d = d + (a[[i]]-b[[i]])^2
      d = sqrt(d)
      return(d)   }
  
  for (j in seq(1:nrow(test))) {
      for (i in seq(1:nrow(train))) {
          dist[i] <- as.matrix(euclideanDist(train[i,], test[j,])) }  #??? train[i,], test[j,]
      
      result <- data.frame(cl, dist)
      #sort the dataframe by distance
      result <- result[order(result$dist),]
  
      #Take the K nearest points
      k_nearest <- result$cl[1:k]
      k_result <- result[1:k,]
  
      n1 <- 0
      for (t in 1:k){
          if (k_nearest[t] == 1){
             n1 <- n1 + 1
          }
          else {n1 <- n1}   }
    
      if (k == n1)
         {test.cl[j] <- 1}
      else if (n1 == 0)
         {test.cl[j] <- 0}
      else if ( mean(subset(k_result, k_result$cl == 1)$dist) < mean(subset(k_result, k_result$cl == 0)$dist) )
         {test.cl[j] <- 1}
      else if ( mean(subset(k_result, k_result$cl == 1)$dist) > mean(subset(k_result, k_result$cl == 0)$dist) ) 
        {test.cl[j] <- 0}
      else  {test.cl[j] <- sample(0:1, 1, replace=T)}   }#for j
return(test.cl) }
```


```{r}
train_data = read.csv('training_data_knn_mp1.csv',header = TRUE)

# Load test data
test_data=read.csv('test_data_knn_mp1.csv',header = TRUE)

# Apply KNN
# Fit KNN with K = 1,2,...,35,50,100,150

# Form sequence of K's 
K=c(seq(1, 35, by = 1), 50, 100, 150)

# Define variables to record 
train_err_rate=test_err_rate=c()

for (i in 1:length(K)) {
  set.seed(456)
  # Fit KNN for K_i and evaluate its performance on training data
  new_mod_train <- modified_knn(train=train_data[,1:2], test=train_data[,1:2], cl=train_data[,3], k = K[i])
  # Fit KNN for K_i and evaluate its performance on test data
  new_mod_test <- modified_knn(train_data[,1:2], test_data[,1:2], train_data[,3], k = K[i])
  # Record error rate for training data
  train_err_rate[i] <- mean(new_mod_train != train_data[,3])
  # Record error rate for test data
  test_err_rate[i] <- mean(new_mod_test != test_data[,3])
}

```


```{r}
# Plot training and test error rates against 1/K.
library(ggplot2)
result=data.frame(K,flexibility=1/K,train_err_rate,test_err_rate)
ggplot(result)+
  geom_line(aes(flexibility,train_err_rate,color="train"))+
  geom_line(aes(flexibility,test_err_rate,color="test"))+
  geom_vline(xintercept = (1/K)[which.min(test_err_rate)],color = "blue", size=0.5,linetype="dotted")+
  labs(x='1/K (Flexibility)',y='error rate',color='Color')

```


```{r}
# Finding the optimal value of K based on test error rate.
result[test_err_rate == min(result$test_err_rate),]

```

## b
Plot training and test error rates against $1/K$. Explain what you observe. Is it consistent with what you expect from the class?    

**Answer**:    
The plot shows that the test error rate and the training rate both decreases as flexibility increases. The plot shows  the optimal k to be when k=1.  This results is consistent with that of the class if optimal k is indeed 1. 

## c
What is the optimal value of $K$ (if there are several such $K$'s pick the smallest one)? What are the training and test error rates associated with the optimal $K$?   

**Answer**:   
The optimal K is found to be at k =1 or k=2.  The test error rate for both is 0.266667 and the training error rate is 0 for both cases. Therefore, to avoid  overfitting that comes with k=1, we select the optimal k to be 2 . 

ISL, Page 25, the caption of Figure 2.7.: In general, as the flexibility of a method increases, its interpretability decreases.
Therefore, when k=2, the model is more interpretable. For example, based on our dataset, sometimes we prefer to use a more interpretable model. In such a case, k=2 would be a better option. However, if a more flexible method is required, then the K=1 is more preferable.  


## Comparison: 
The first criterion gave an optimal K=33 which is  less flexible and has lower test error rate.The optimal k for the second criterion gives a k which is more flexible and has higher test error rate. Comparing the test error rates, the first criterion may be a better approach to classifying the data using k nearest neighbours.
